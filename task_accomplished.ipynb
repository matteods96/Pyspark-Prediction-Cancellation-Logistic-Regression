{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f507ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d3049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e86693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc681e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fb970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"C:\\\\Users\\\\Andrea\\\\Desktop\\\\Flightright Coding Challenge\\\\2. Prediction Model\\\\dataframe.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0d716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id='66860ae6', month_interaction_count='41', week_interaction_count='9', day_interaction_count='0', cancelled_within_week='1'),\n",
       " Row(user_id='249803f8', month_interaction_count='25', week_interaction_count='9', day_interaction_count='2', cancelled_within_week='0'),\n",
       " Row(user_id='32ed74cc', month_interaction_count='21', week_interaction_count='2', day_interaction_count='1', cancelled_within_week='1'),\n",
       " Row(user_id='7ed76e6a', month_interaction_count='22', week_interaction_count='5', day_interaction_count='2', cancelled_within_week='0'),\n",
       " Row(user_id='46c81f43', month_interaction_count='32', week_interaction_count='8', day_interaction_count='2', cancelled_within_week='0'),\n",
       " Row(user_id='cf0f185e', month_interaction_count='26', week_interaction_count='4', day_interaction_count='0', cancelled_within_week='1'),\n",
       " Row(user_id='568275b3', month_interaction_count='29', week_interaction_count='5', day_interaction_count='1', cancelled_within_week='1'),\n",
       " Row(user_id='86a060ec', month_interaction_count='33', week_interaction_count='7', day_interaction_count='1', cancelled_within_week='1'),\n",
       " Row(user_id='c0c07290', month_interaction_count='35', week_interaction_count='10', day_interaction_count='0', cancelled_within_week='0'),\n",
       " Row(user_id='709dc1da', month_interaction_count='36', week_interaction_count='11', day_interaction_count='1', cancelled_within_week='0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ea3ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "| user_id|month_interaction_count|week_interaction_count|day_interaction_count|cancelled_within_week|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "|66860ae6|                     41|                     9|                    0|                    1|\n",
      "|249803f8|                     25|                     9|                    2|                    0|\n",
      "|32ed74cc|                     21|                     2|                    1|                    1|\n",
      "|7ed76e6a|                     22|                     5|                    2|                    0|\n",
      "|46c81f43|                     32|                     8|                    2|                    0|\n",
      "|cf0f185e|                     26|                     4|                    0|                    1|\n",
      "|568275b3|                     29|                     5|                    1|                    1|\n",
      "|86a060ec|                     33|                     7|                    1|                    1|\n",
      "|c0c07290|                     35|                    10|                    0|                    0|\n",
      "|709dc1da|                     36|                    11|                    1|                    0|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e55a3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- month_interaction_count: string (nullable = true)\n",
      " |-- week_interaction_count: string (nullable = true)\n",
      " |-- day_interaction_count: string (nullable = true)\n",
      " |-- cancelled_within_week: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94265041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StringType, BooleanType, IntegerType, FloatType, DateType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ca97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coltype_map = {\n",
    "    \"user_id\": StringType(),\n",
    "    \"month_interaction_count\":IntegerType(),\n",
    "    \"week_interaction_count\": IntegerType(),\n",
    "    \"day_interaction_count\": IntegerType(),\n",
    "    \"cancelled_within_week\": IntegerType(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9171d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- month_interaction_count: integer (nullable = true)\n",
      " |-- week_interaction_count: integer (nullable = true)\n",
      " |-- day_interaction_count: integer (nullable = true)\n",
      " |-- cancelled_within_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"month_interaction_count\", col(\"month_interaction_count\").cast(IntegerType())) \n",
    "df = df.withColumn(\"week_interaction_count\", col(\"week_interaction_count\").cast(IntegerType())) \n",
    "df = df.withColumn(\"day_interaction_count\", col(\"day_interaction_count\").cast(IntegerType())) \n",
    "df = df.withColumn(\"cancelled_within_week\", col(\"cancelled_within_week\").cast(IntegerType())) \n",
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa867fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+----------------------+---------------------+---------------------+---------------+\n",
      "| user_id|month_interaction_count|week_interaction_count|day_interaction_count|cancelled_within_week|       features|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+---------------+\n",
      "|66860ae6|                     41|                     9|                    0|                    1| [41.0,9.0,0.0]|\n",
      "|249803f8|                     25|                     9|                    2|                    0| [25.0,9.0,2.0]|\n",
      "|32ed74cc|                     21|                     2|                    1|                    1| [21.0,2.0,1.0]|\n",
      "|7ed76e6a|                     22|                     5|                    2|                    0| [22.0,5.0,2.0]|\n",
      "|46c81f43|                     32|                     8|                    2|                    0| [32.0,8.0,2.0]|\n",
      "|cf0f185e|                     26|                     4|                    0|                    1| [26.0,4.0,0.0]|\n",
      "|568275b3|                     29|                     5|                    1|                    1| [29.0,5.0,1.0]|\n",
      "|86a060ec|                     33|                     7|                    1|                    1| [33.0,7.0,1.0]|\n",
      "|c0c07290|                     35|                    10|                    0|                    0|[35.0,10.0,0.0]|\n",
      "|709dc1da|                     36|                    11|                    1|                    0|[36.0,11.0,1.0]|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assemble all the features with VectorAssembler\n",
    "required_features=['month_interaction_count','week_interaction_count','day_interaction_count']\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "\n",
    "transformed_data = assembler.transform(df)\n",
    "transformed_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c625dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 9\n",
      "Test Dataset Count: 1\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "(training_data, test_data) = transformed_data.randomSplit([0.6,0.4], seed =2020)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a59c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+----------------------+---------------------+---------------------+---------------+--------------------+--------------------+----------+\n",
      "| user_id|month_interaction_count|week_interaction_count|day_interaction_count|cancelled_within_week|       features|       rawPrediction|         probability|prediction|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+---------------+--------------------+--------------------+----------+\n",
      "|c0c07290|                     35|                    10|                    0|                    0|[35.0,10.0,0.0]|[-1.7914618395553...|[0.14289359118091...|       1.0|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+---------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'cancelled_within_week', maxIter=10, regParam=0.1,\n",
    "                        elasticNetParam=1, threshold=0.5)\n",
    "lrModel = lr.fit(training_data)\n",
    "lr_predictions = lrModel.transform(test_data)\n",
    "lr_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c9f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=lr_predictions['user_id','rawPrediction','probability','prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b70929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+\n",
      "| user_id|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "|c0c07290|[-1.7914618395553...|[0.14289359118091...|       1.0|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb73e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
